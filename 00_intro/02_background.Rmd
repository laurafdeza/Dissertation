# 2. Background
## 2.1. Anticipation
Anticipation alleviates the load of constantly processing the information that we receive from the world. At a large scale, we anticipate what is going to happen in movies and books. At smaller scales, we anticipate movements in sports, other drivers' actions, and we also anticipate what we read and what we hear. In non-linguistic auditory domains, like music, we anticipate the melody through pitch and rhythm. In non-linguistic visuospatial anticipation, we use the position and movement of objects to anticipate how long it will take an object to reach a certain point, or to anticipate the trajectory it is following. Finally, in linguistic anticipation, we use cues of different kinds (contextual, phonological, morphosyntactic) to predict oncoming linguistic information. Predictive processing in language is especially efficient in the L1, and mediated by factors such as L1 transfer, L2 proficiency, or working memory in the L2. These factors may interact with each other to generate successful or unsuccessful predictions.

### 2.1.1. Non-linguistic anticipation
#### 2.1.1.1. Auditory anticipation
There is abundant evidence supporting a cognitive association between different cognitive domains in an auditory modality. Here, I will focus on the relationship between language, rhythm and pitch. Rhythm and pitch are essential in music, and thus music is the usual framework used to make research on those abilities. Importantly however, the use of a common framework to conduct research on those auditory abilities does not mean they are inseparable. Previous research suggests that pitch-deafness and beat-deafness are dissociated [@phillips2013amusic]. Following this rationale, pitch and rhythm abilities are independent although they interact when put into practice for music perception, processing and production. The independence and interrelation between two auditory abilities, as well as findings on the relationship between pitch and language, and rhythm and language, open the possibility that anticipation in the auditory domain is also underpinned by a shared network of domain-general mechanisms.
Offline studies on the relationship between music and language show that higher musical abilities yield more effective reading comprehension skills in children [@anvari2002relations]. Behavioral online studies have revealed that musical abilities influence positively the production and perception of linguistic sound structures in L2 learners [@slevc2006individual]. These results have been replicated in neurocognitive studies in the same population [@marques2007musicians], as well as in children [@magne2006musician] and adults [@wong2007musical]. Similarly, other neurocognitive studies reveal that musicians process speech in a more enhanced way and more synchronously to its onset than non-musicians [@musacchia2007musicians].      
The influence of music in language starts early during childhood. Regarding syntax, @jentschke2014neural found that music and language follow a similar time course of acquisition of syntactic regularities. @dege2011effect compared the effects of a music program and a phonological skills program on children’s phonological awareness, and they found that phonological awareness of small and large phonological units improved from a pre-test to a post-test for both treatment groups, while no improvement was observed for a control group. In a similar fashion, @lebedeva2010sing explored whether infants were able to discriminate phonetic from melodic patterns in songs. Results indicated that 11-month-old infants could discriminate between 4-note sung melodies, but the infants did not show any preference towards familiar or unfamiliar spoken strings containing a syllable order change when the intonation contour remained stable for both strings. However, when the melodies were presented without the phonetic cue, and thus were not sung only instrumental, infants did not show any preference, suggesting that they were not discriminating the melodies based on their melodic contour alone. Taken together, the findings of the tasks @lebedeva2010sing deployed suggest that infants use the phonetic content of melodies to process a melodic line, even if they are not able to understand fully what they are hearing. Findings like the one by @lebedeva2010sing and the resemblance in processing between music and language have even led some scholars to suggest that language is processed as a type of music by the early developing brain [@koelsch2005towards].    
The music-language association found in children is also reported in adult native speakers and L2 learners. In amusic individuals, that is, individuals who have lost or with impaired musical capacity due to different reasons [e.g., congenital, accidents, @benton1977amusias], ERPs have shown that syntactic irregularities in music (i.e. notes incompatible with the scale used) and language (i.e., disagreement of number) produce similar reduced or non-existent ERAN and LAN responses in comparison to healthy individuals, while processing semantic irregularities (e.g. "Linda is feeding two kites") did not produce any changes, as shown by a typical N400 signal [@sun2018syntactic]. @yu2017shared conducted a fMRI study particularly designed to test how different components of music and first language are associated. They compared the results from a group of participants undergoing music training group and a control group in language tests (an animal-word cancellation test and an onset cancellation test) and music tests (an interval test and a rhythmic test). The training group performed better in all tests, and what is more, the accuracy in the animal-word cancellation test and the accuracy in the interval test were positively correlated.    
With regard to L2 learners, multiple studies have tested how musical abilities and L2 performance at different levels. Between pitch in particular and the segmental level of speech, @mokari2018perceptual studied whether there was any kind of association between general musical abilities and L2 vowel learning with L1 Azerbaijani learners of English. The participants were split into control group and experimental group, who underwent phonetic training. Both groups completed music ability tests (chord analysis, pitch change and tonal memory) and linguistic tests (discrimination and production). Results point that the increased accuracy in discrimination and production of L2 sounds from the pre- to the post-test is not related to the musical ability in general, although tonal memory was significantly correlated to the gained skill to discriminate L2 vowels. At the suprasegmental level, the influence between music and language is bidirectional. On the one hand, musical knowledge of pitch facilititates perception of L2 tones in musician speakers of non-tonal L1s [@ngo2016effects]. On the other hand, non-musician speakers of L1 tonal languages have increased ability in pitch and music perception in general [@bidelman2013tone].     
Regarding rhythm and L2 abilities, @swaminathan2018explaining explored if and how rhythm perception was associated with reading abilities in both the L1 and L2, and they found however that better rhythm perception was not associated with increased reading abilities in either type of language. In speech, suprasegmental information like stress is also linked to musical abilities. @cason2019rhythmic investigated how rhythmic abilities were related to the perception and production of L2 stress placement. Rhythmic production scores were a reliable predictor of L2 stress placement.     
In sum, research suggests that at least some cognitive faculties underlying music and language are shared. Sharing the faculty might be determined by the area we are focusing on, like speech vs. reading. Previous research also suggests that anticipation is an important mechanism in both realms, especially in relation to rhythm, but to the best of my knowledge, no study so far has tested whether this processing mechanism is also interdependent to each domain or shared. Positive correlation between linguistic anticipation and other types of anticipation common in music, such as rhythmic or tonal, could indicate that there are domain-general predictive cognitive mechanisms underpinning auditory anticipation as a whole.     

_Pitch_   
Pitch is the frequency associated to a sound wave; this frequency places the sound within a scale ranging from low to high in perception [@klapuri2006introduction]. Pitch in music plays a similar role to the one it plays in language. In language, a listener must process the melodic contour of speech, that is, the changes up and down of pitch in order to understand the meaning conveyed by prosody [@dilley2005phonetics].  For example, a rising pitch contour signals a question (“Coffee?”), while a falling intonation signals a statement (“Coffee.”). Apart from the phrasal and sentential intonation, pitch also affects tones and predominance against other syllables within a word. In music, pitch affects the note we hear.    
Pitch is not only a common acoustic correlate in speech and music. Knowledge and perception abilities of pitch in one of the two auditory domains translates in many but not all cases into increased perception abilities in the other domain [@bidelman2013tone; @chan2019lexical]. This relationship shows between perception of pitch in tonal languages and absolute pitch in music [e.g., @chua2014effect; @deutsch2009absolute; @ngo2016effects]. The relationship does not show in other areas, like relative pitch and L1 tone knowledge [@ngo2016effects]. Atypical populations have also provided evidence of a relationship between pitch and speech [in amusia, @patel2008speech; in Williams Syndrome, @martinez2014pitch].     
In typical populations, L1 speakers of tonal languages are better and faster at discriminating and detecting absolute pitch in music [e.g., @chua2014effect; @deutsch2009absolute; @tsukada2015perception], since tones in music depend on pitch, like tones in many languages. Music in different cultures around the world vary on how their scales are organized [as well as on the patterns of strong-weak pulses, @morrison2009cultural], but the basis for the scales is always pitch. Pitch collections may be different from one music culture to another, but the range is shared. @bidelman2013tone tested how L1 Cantonese listeners, non-tonal speaking musically-trained participants and a non-tonal speaking non-musician control group performed in a three-alternative forced-choice task, an auditory inspection time paradigm, a short-term pitch memory task and a melody discrimination task to compare their measures of auditory pitch acuity, music perception, and general cognitive ability. The Cantonese speakers outperformed the control group on most pitch and music perception measures, and performed similarly to the musicians’ group. Because of the good performance of the Cantonese group, who had no musical knowledge, the authors suggested that music and language abilities transfer bidirectionally. Additionally, musicianship contributes to better lexical tone perception even in a tonal language [@tang2016musical].   
In contrast to the studies described so far, other studies have failed to obtain evidence that there is a music-language association in typical populations. @chan2019lexical studied the creation of L2 tone-segment connections in musicians and non-musicians. Their results indicate that musical experience did not promote tone-segment connection in a tonal L2 when the L1 was non-tonal, but a tonal L1 did promote L2 tone-segment connection and the creation of a rule-like system of L2 tone information. Likewise, only musical experience can be beneficial in relative pitch perception in music, while tonal-language experience has no effect in relative pitch perception performance [@ngo2016effects]. From this set of results, it is possible to deduce that there is a limit in the reciprocal influence between music and language (musical and linguistic pitch for the purpose of this project), but where this limit lies has not been ascertained.   
An area the relationship between pitch and language is clearer is atypical populations. Research on atypical populations like amusics [individuals with reduced or lost musical ability, such that comprehension and production of music, and ability to read and/or write musical notation are impeded, @pearce2005selected] or individuals with Williams Syndrome [WS, a rare genetic disorder that results in cognitive impairments and other biological alterations, @jones1975williams] has produced larger evidence of a connection between pitch and language. In a study in amusia, @patel2008speech observed that amusic individuals distinguish changes in pitch but are unable to detect the direction of the change, such that they can discriminate changes in intonation, but cannot really tell what the prosodic change means. This ability to perceive pitch and its changes is probably one of the most studied aspects of the relationship between music and language. @martinez2014pitch investigated whether children with WS processed pitch in music and language through common mechanisms. For that purpose, the WS children and typically developing children completed a musical pitch discrimination task, a short-item discrimination task and a long-item discrimination task. The WS group performed well above chance in all tasks, but their scores were significantly lower than those of the control group of typically developing children. In the case of the experimental group, the scores for the musical pitch discrimination-task and the short-item discrimination task were also correlated.   
In sum, some areas of music and language are connected, but influence from music to language and vice versa may be blocked. What these specific areas are is not completely known. While the shared processing mechanisms between music and language have been previously researched, no study so far has looked into how the anticipation mechanisms may be shared by the different auditory domains, or how the different acoustic correlates may affect anticipation in these domains. Separately, there is evidence for the existence of prediction processes in music [e.g., @salimpoor2015predictions] as well as in language. Like in language, anticipation in music can be cued through syntactic structure [@sammler2013syntax] or acoustic properties [@loui2007harmonic]. Anticipation has been particularly examined in rhythm, as synchronization to a rhythm is intrinsically keeping track of the interval structure, and therefore anticipation of the next event.     

_Rhythm_    
Rhythm is defined as a pattern of recurrent time intervals that usually occur periodically [@berlyne1971aesthetics]. This periodic nature allows to predict the start of an interval or the next recurring event based on what has already been perceived [@fraisse2013rhythm; @martin1972rhythmic]. Given hence the nature of rhythm, synchronizing to a rhythm is basically anticipating when the next interval is coming and acting accordingly. One of the most natural ways to accompany the predicted rhythm is through movement, like body movement. This ability to anticipate events and synchronized with them based on rhythmic patterns is already evident in babies as young as one year of age in their rocking to musical rhythms [@fraisse1949aptitudes]. And by the age of three or four we are able to tap along metronome beats. This sensorimotor behavior is usually based on anticipation of when the next beat is coming, especially at slow tempi [@nozaradan2016individual; @van2015sensorimotor], and the anticipation grows in accuracy with years of musical training [@nozaradan2016individual]. Prediction of oncoming beats is also present in rhythms with tempo changes [@van2015sensorimotor].   
The ability to synchronize to beats or changes in rhythm is conditioned by (1) motor limits, (2) the fractured memory of a slow sequence, (3) sensory difficulties at perceiving successiveness at fast tempi [@bartlett1959synchronization], and (4) the complexity of the rhythm [@fraisse2012anticipation].However, the adaptation is fast, since it can take as few as three consecutive taps accompanying each a consecutive beat in a novel rhythm to achieve a relative simultaneity [@fraisse2012anticipation]. The rhythm structure affects the synchronization pattern, as some tempo speeds are easier to adjust to than others. When the interval between beats is on or under 1500 ms, the synchronization behavior tends towards anticipation of the next beat, while this anticipation rate starts to decrease with beat onset intervals of 1800 ms [@miyake2004two].   
Rhythms can happen in multiple formats, so they can be perceived multisensorily too. For example, rhythms can be visual, like the bouncing of a ball, or heard, like a march. So being impaired in a hearing, for instance, does not mean that an individual is unable to perceive and synchronize to rhythms. Within the auditory modality, music is probably the most obvious instance of rhythm, but other domains such as language also follow rhythmic patterns. Rhyhtmic abilities across domains have been shown to be associated. Rhythmic musical abilities are associated with linguistic performance in children [@carr2014beat], L2 speakers [@cason2019rhythmic], and atypical populations [@lagrois2019poor].    
Rhythms can be perceived visually, as research with deaf individuals show. @iversen2015synchronization investigated how the accuracy of visual timing abilities and rhythmic perception are affected by developmental experience by comparing deaf and hearing adults in a tap synchronization task with three types of isochronous rhythmic stimuli: a flashing visual square, an animated bouncing ball and auditory tones. Results revealed that synchronization driven by a silent moving visual stimulus can be as accurate as that driven by sound, and that deaf individuals could synchronize their movements as well or even better than hearing individuals when presented with visual stimuli. However, rhythms are perceived primarily through our auditory pathways, like speech.   
Rhythms perceived auditorily are the ones that have been used to research hearing populations, regardless of whether the studies also included visual tasks, like reading. @swaminathan2018explaining studied the existence of an association between rhythm perception and reading ability in adults in their L1 and L2 and found no evidence. In contrast, @cason2012rhythmic did find that on-beat sequence primes yielded faster L1 speech processing than off-beat primes in adults through EEG recordings. The contradictory results obtained from these populations suggest that the effects of rhythm in hearing adults may not be as extensive as in deaf individuals in the visual domain. Maybe because hearing individuals do not need to hone their visual perception and processing abilities. Alternatively, there might be some “loss” in the extrapolation of rhythmic abilities from a visual domain to an auditory domain or vice versa, causing the relationship between rhythm and language in different modalities to be weaker. This conclusion would be supported by findings suggesting that within a modality, the relationship between rhythm and language, in this case speech, is stronger.   
Like in tone and pitch, there is evidence that rhythm is associated with language in children, L2 speakers, and atypical populations (beat-deafness, dyslexia and WS) within the auditory modality. Starting with children, @carr2014beat had their typically developing children complete a series of linguistic, verbal memory, rhythmic and musical in general tasks to investigate the relationship between rhythmic synchronization and the encoding of syllable envelops in pre-schoolers. Those children who synchronized better also had better perceptual and cognitive language skills. They were faster at naming objects and color, and their neural encoding of the speech envelope was also better. Encoding of sound is also favored by rhythmic perception in the L2 experience. @cason2019rhythmic and @bhatara2015foreign tested stress placement and rhythmic abilities in L1 French speakers and L2 Spanish. The rhythmic production scores of the participants predicted correct placement of nuclear stress in a L2 lexical stress imitation task; and rhythm perception was actually positively associated with L2 learning experience. These studies suggest that rhythm is indeed related to speech in typical populations within a modality, and specifically, within the auditory domain.    
Studies with atypical populations have also yielded positive results supporting a relationship between heard rhythms and speech. @lagrois2019poor tested beat-deaf individuals, that is, individuals who have a documented deficit in tracking musical beat, to analyze how they synchronized to speech. Participants had to tap along sentences in three conditions: regularly spoken, naturally spoken and sung, and were compared to a matched control group. The beat-deaf group showed more variability in its tapping regardless of the condition. The irregularity remained when they tapped to a metronome or at their own pace and was larger than in the typical non-musician control group. These results show that the deficiency in time-keeping mechanisms might underlie both domains rather than being domain-specific. In the same vein, @persici2019rhythmic supported this conclusion when they compared the anticipation abilities of children with developmental dyslexia and typically developing children, and the results showed that children with dyslexia achieved lower scores than their peers in morphosyntactic and rhythmic processing. Therefore, noun prediction based on this information looked alike for both groups, but phonologically- and/or grammatically-based anticipation was hindered for the dyslexic children. Similarly, @sun2018syntactic explored brain responses to syntactic violations in music and language in amusics. Brain responses to incongruities in both domains were reduced, but brain responses at later stages of processing, that is, during semantic processing, were unaffected or showed the same pattern as brain responses in typical individuals to semantic incongruencies. Based thus on previous findings suggesting the delayed anticipation in rhythm, phonology and morphosyntax, it is not unlikely that structure-based prediction is contingent to a cognitive mechanism common to the auditory mode in general, rather to domain-specific abilities.   
From the previous investigations on rhythm, we can see that synchronization is basically prediction of the rhythm timings. The findings on the relationship between rhythmic abilities and linguistic competence suggest that this relationship might only surface in certain cases, and that extrapolation from modalities may affect how strong the relationship is. Some of the factors conditioning the existence of a connection between the two domains are atypical development, early stages of cognitive development, nature of L1, or linguistic area in L2 learning. Investigations from other auditory areas, such as speech anticipation, may clarify if and how rhythmic abilities and linguistic abilities are related. One of the main visuospatial cues related to rhythm is movement. Taking into account how rhythm imbibes from visual and spatial cues might be helpful in establishing the nature of the rhythm-language relationship, as keeping a rhythm requires gauging of space in order to be timed correctly. And movement, like language, is entrenched in our everyday life. We use movement to respond to virtually everything, not only rhythms. When driving, we anticipate what other drivers or pedestrians are going to do and adjust accordingly by making foot, manual and head movements. When catching an object in a free fall, we anticipate (more or less successfully) the trajectory and the speed to move our arm and hand fast enough and try to grab the object before it hits the floor.    

#### 2.1.1.2. Visuospatial anticipation
Visuospatial abilities refer to the ability to process, work with and remember information in space perceived visually. The visuospatial domain can interact with language through written texts in the way we read and write [e.g. @schmalz2015getting], but it can also interact in how we process information. Both typical and atypical populations have shown evidence of how the spatial reference system interacts with linguistic representation of space in comprehending and producing language [e.g. @bochynska2020spatial; @landau2005parallels].    
In written language, we can write and read left to right, or right to left, and also up downwards, depending on the language. In any language, be it alphabetical, abjad, syllabary, or any other type of language, there are strict rules on how their different components should be arranged spatially. In sign languages, space cognition may be important both in lip reading and in signed communication. Even in normal speech, language reflects how space is visually perceived; or from the opposite point of view, maybe how space is visually perceived determines how language conceptualizes space. In any case, if a lay person is asked in a European country where each cardinal direction lies, they may have a hard time guessing it. That is because their space perception is mainly egocentric and hence defined by concepts such as up, down, next to, or behind in reference to the speaker. In other languages, such as the Aboriginal Australian Guugu Yimithirr group, geocentric referents are taken, so position and directions are expressed in terms of north, south, east and west. Even within a single language, we need to create a representation of space to talk about concepts such as distance, and thus use the demonstrative system correctly. This idea is actually supported by empirical evidence that linguistic and non-linguistic spatial representations rely on a common axis-structure, at least in English [@crawford2000linguistic; @huttenlocher1991categories].    
In typical populations, interference of visuospatial abilities in language has been proven in languages with orthographic depth, like English. Orthographic depth is the linguistic phenomenon by which written words need to be processed semantically in order to be pronounced correctly [@liberman1980orthography]. Thus, English is a deep language in the sense that the same letter or string of letters can be pronounced differently depending on the word, so the speaker needs to process the word semantically first to know which pronunciation to assign to the letter or string of letters [@katz1992reading; @schmalz2015getting]. In contrast, languages like Italian or German are shallow, as a letter or string of letters tend to be pronounced the same regardless of its position in a word or the word they appear in. In a meta-study of visuospatial interference in language, @estes2018comprehensive found that visuospatial interference was larger in deep languages, while it was nearly non-existent in shallow languages.     
Associations between the linguistic and the visuospatial systems also manifest in different pools of atypical populations. But for a few exceptions [@lukacs2007spatial], WS has been particularly prolific in providing evidence of a connection between visuospatial representation and linguistic performance [e.g., @farran2016impaired; @landau2005parallels). @landau2005parallels set to investigate if WS individuals reflect in linguistic and non-linguistic tasks having structured representations of reference systems. While certain patterns present analogies between the experimental and the control typically developing children group, children and adults with WS showed poorer performance compared to controls in trials where they had to locate orally and by gestures objects farther apart from the reference object, especially when located on the horizontal axis. While the results suggest that WS individuals use a structured system of axes as reference to represent space, the system lacks certainty in some areas, namely the horizontal axis, that translates into their comprehension and production of spatial linguistic terms. @phillips2004comprehension compared the visuospatial representation system of WS against a group of children with moderate learning difficulties. WS participants made more errors in comprehending those sentences containing a spatial component than both the typically developing controls and the population with moderate learning difficulties. The results from this population suggest that WS problems stem from a deficit in processing of spatial descriptions rather than from semantics [@laing2007comprehension]. This deficit influences in turn the interaction between language processing and spatial abilities.   
Research with other atypical populations, i.e. autism and dyslexia, has led further support to the conclusions reached after exploring the vision/space-language relationship in WS. @bochynska2020spatial conducted a series of eye-tracking, offline and production experiments with autistic individuals to test the nature of the relationship between space representation and spatial language. The particular target structure that served as object of the studies was the axial reference system.  In terms of spatial language production, autistic participants used a smaller set of terms to describe the images thew saw regardless of the condition, whereas typical participants used a wider range of terms for trials where the circle was placed or moved along the axes of the square. Additionally, the autistic group was significantly less accurate in the descriptions. The eye-tracking data showed similar patterns for both groups but were noisier for the autistic participants. This noise is probably due to the uncertainty provoked by weaker representations of spatial terms. Furthermore, Bochynska et al. suggest that weaker representations of spatial terms may be linked to less efficient processing of spatial language, similarly to the case of WS.    
Dyslexia has also provided some evidence of the existence of the relationship between space and language. This relationship, however, is more obvious in dyslexic children than in dyslexic adults. @giovagnoli2016role tested children with developmental dyslexia in order to explore how reading and visuospatial abilities change according to the educational stage. The younger dyslexic children performed worse in a mental rotation task, a more-local visuospatial task, a more-global visual-perceptual task, and a visual-motor integration task as compared to age-matched typically developing children. The older dyslexic children had caught up with their age-matched typically developing peers in all but one task, the visual-perceptual task. The visuospatial abilities therefore may be contributing to reading capacities as a more global perception strategy. Interestingly, @von2004dyslexia found that visuospatial deficits were also present in adults and older teens, although to a lesser extent. In their studies, visuospatial abilities were inferior in dyslexic individuals than in typical controls, except for global visual spatial tasks, in which dyslexics were actually enhanced. Another ability in which dyslexic individuals may be superior is in analytic spatial abilities, at least in older children and younger teens [@duranovic2015dyslexia].
In sum, research on atypical populations in particular has yielded a hodgepodge of mixed results.  The impact of the visuospatial domain in language may be larger in some populations, such as WS [@landau2005parallels], than in other, such as dyslexics [@von2004dyslexia]. And certain linguistic deficits may even be linked to increased visuospatial abilities [@duranovic2015dyslexia]. However, most of this research was performed offline [with the exception of @bochynska2020spatial] or on subjects whose age range was too broad [@phillips2004comprehension]. Focusing on online measures, such as eye-tracking, can reveal more fine differences in information processing caused by the interaction between the linguistic domain and the visuospatial domain. While it is possible that individual differences in visuospatial representation and processing may affect atypical populations more visibly [@bochynska2020spatial], online measures can provide further insight into how processing of the visuospatial domain is related to linguistic processing in typical populations too, instead of just focusing on representation systems and offline measures.    
Anticipation offers a good new perspective to further investigate how the visuospatial and linguistic domains are associated, as both in language and in visuospatial domains humans anticipate events and measures have to be online. In other words, data are not stained by processing yet. In the visuospatial domain, when a moving object disappears and is bound to appear again, we tend to anticipate when it will do so by adjusting our eye movements to the speed at which the target was going before it disappeared [@bennett2004predictive; @bennett2005timing; @bennett2006combined; @orban2006evidence]. In a real world environment, driving is a good example of anticipation put into practice. Fronto-central negative ERP activity suggests that drivers anticipate randomized car accidents in a passive, ecological driving simulation [@duma2017driving]. In order to anticipate and try to avoid the threat of a car accident, drivers use somatosensory and visual cues [@morando2016drivers]. When exposure to these cues is interrupted, the drivers’ ability to anticipate a hazard on the road decreases [@borowsky2016effects]. If humans therefore anticipate both in visual and auditory modalities, it is possible that there is some relationship between the prediction abilities in cognition in general. If there were some type of positive relationship, this association would suggest that the supporting cognitive mechanisms of anticipation are primarily not only domain-general, but domain- and modality-general.     

### 2.1.2. Linguistic anticipation
In this project, I adopt a predictive processing view of language. That is, language processing is facilitated and preceded by prediction of linguistic information based on the already available information, which acts as prediction cues. The underlying mechanisms of how prediction works are still a matter of debate. Linguistic prediction might well be domain-specific, but research on populations with reduced/loaded executive resources resulting in underachieved prediction suggests that linguistic anticipation is actually related to more general cognitive resources. Thus, children’s executive resources do not reach maturity until early adulthood [@davidson2006development; @de2010developmental]; this lack of cognitive development has been used to account for their an inability to predict linguistic correctly [e.g., @friedrich2005phonotactic; @gambi2018development]. On the other end, the executive functions in adults over 45 years of age have already declined enough to slow prediction [e.g., @dagerman2006aging; @federmeier2019s]. In a different context, L2 learners [e.g., @lew2010real; @mitsugi2016use] load the pool of executive functions that they would otherwise use for anticipation in their L1 [@linck2014working], although in their case, lack of prior linguistic experience also affect their ability to generate linguistic predictions [@cuetos1996parsing]. Lack of enough language experience as a factor determining anticipation performance could naturally be applied to children to. Similarly, even adults in their L1 constantly readapt their expectations based on previous linguistic input [e.g., @levy2008expectation; @ryskin2017verb].     

#### 2.1.2.1. L1 speakers
Just like in any other realm of life, humans tend to anticipate linguistic information. We anticipate semantic [@altmann1999incremental] and morphosyntactic [@gruter2016l2; @lew2010real] information by using morphosyntactic [@gruter2016l2], syntactic [@linzen2016uncertainty], semantic [@kamide2003time; @pozzan2016semantic] and phonological cues. Relevant to my dissertation, the phonological cues that speakers may use to anticipate in their L1 are numerous: coarticulation [@salverda2014immediate], intonation [@nakamura2012immediate; @weber2006role], lexical stress [@correia2013word; @sagarra2018suprasegmental], pauses between clauses [@hawthorne2014pauses; @kjelgaard1999prosodic], vowel duration [@rehrig2017acoustic], and tone [@roll2015neurolinguistic; @roll2011activating].    
Linguistic prediction allows listeners to anticipate upcoming words and regions [@brennan2019hierarchical; @yang2019clause], and word endings [@roll2010word; @sagarra2018suprasegmental; @soto2001segmental]. Here, I will focus briefly on suffix prediction. Predictions of morphosyntactic elements can be generated through determiners [in Dutch, @huettig2016prediction; in Spanish, @dussias2013gender; @lew2010real; in German, @hopp2013grammatical; in French, @dahan2000linguistic]. Determiners can provide information about number [@marull2017second], gender [@dahan2000linguistic], and case [German @hopp2015semantics; Japanese, @mitsugi2016use].     
Within the very word containing the suffix to be predicted, phonological information such as tone or lexical stress can cue the generation of the correct anticipation of word endings. Swedish speakers use tonal cues to predict noun number [if low tone, then singular, _fisken_ 'fish~[SG]~'; if high tone, then plural, _fiskar_ 'fish~[PL]~,' @roll2010word; @soderstrom2015using; @roll2013word] and verb tense [if low tone, then present, _skrämmer_ 'I scare'; if high tone, then past, _skrämde_ 'I scared,' @soderstrom2012processing; @roll2015neurolinguistic]. Similarly, Spanish speakers use lexical stress to predict verbal tense [if first syllable stressed, then present; if unstressed, then past: _CANta_ 'he sings' vs. _canTÓ_ 'he sang,' @sagarra2018suprasegmental] and noun ending [ _PRINcipe_ 'prince' vs. _prinCIPIO_ 'beginning,' @soto2001segmental]. Finally, English natives use vowel duration to predict voice [if shorter vowel duration, then active: 'the girl was pushing the boy'; if longer vowel duration, then passive: 'the girl was pushed by the boy,' @rehrig2017acoustic].    
Although findings on anticipation in L1 show that typical L1 speakers tend to perform predictive language processing, it is unclear how individual variability in cognitive capacities, i.e. working memory, can affect the efficacy of the predictions. @huettig2016prediction found through eye-tracking that the variance between participants in fixating on the correct target noun based on gendered article in Dutch could be accounted for working memory. However, @sagarra2018suprasegmental found also by means of eye-tracking that working memory played no role, or a marginal one, in L1 Spanish speakers’ capacity to anticipate verbal tense based on lexical stress. Similarly, @otten2009does found no effect of working memory on L1 anticipation in an ERP study. It is difficult to interpret these results, as whereas the technique was the same in the eye-tracking studies, the domains under research cannot explain the differences. @sagarra2018suprasegmental found none focusing on the role of prosodic cues, and @huettig2016prediction found a working memory effect focusing on the role of morphosyntactic information, but @otten2009does found no effect either also focusing on the role of morphosyntactic information but using a different online method. It is possible that different levels of linguistic complexity interact with behavioral or brain activity performance, and only in certain cases the impact of differences in working memory capacities are visible in linguistic anticipation. However, the scant amount of research on the interaction between linguistic anticipation and working memory capacities forces to make interpretations of the findings so far with caution.

#### 2.1.2.2. L2 speakers
Anticipation in L2 speakers is controversial because different studies revealed different findings. Particularly in morphosyntactic anticipation, some studies find that L2 speakers can generate predictions [@dussias2013gender], while others have found they cannot [@hopp2016learning], or only in certain situations [@lew2010real] or specific levels of proficiency [@sagarra2018suprasegmental]. Two of the factors that could account for the variability in L2 anticipation performance and that often appear together are proficiency and cross-linguistic differences. Another factor that could explain the ability to predict in an L2 or not is whether morphosyntax is the cue, the outcome, or both.
It is difficult to disentangle the influence of L1 transfer and L2 proficiency because they are often confounded variables in studies lacking language pairs with different L1s. Furthermore, the combination of L1 transfer and L2 proficiency can happen at different levels of linguistic processing, from the smaller parts like morphology to higher-order levels. At higher-order levels research is very scant. @gruter2013l2, @gruter2014role and @gruter2016l2 explored whether L1 speakers of Korean or Japanese could anticipate the correct oncoming syntactic structure in the discourse in L2 English based on coreference. While L2 speakers were sensitive to the cue, they did not use it to anticipate the structure of the syntactic event. At lower levels where phonology and morphosyntax are involved, there is some more research.   
Anticipation of and based on morphosyntax in an L2 has been tested primarily with gender [e.g., @hopp2013grammatical]. As opposed to L1 anticipation, however, morphosyntactic anticipation in the L2 is determined by the listener's proficiency in the L2 [@sagarra2018suprasegmental] and their L1. The L1 linguistic system can both help [@dussias2013gender] and hinder [@dupoux2008persistent] L2 processing. 
In L2 morphosyntactic anticipation, the gender in the determiner is a reliable cue to gender suffixes in nouns, although some restrictions apply. In L2 German, determiner gender is reliable if the L2 speaker uses the gender system target-like [@hopp2013grammatical; @hopp2016learning]. In L2 Spanish, predictions may be generated if the L1 also has a gender system [@dussias2013gender], or in case the L1 lacks a gender system, if the predictions are to be made on novel nouns [@gruter2013l2], or if the nouns are highly frequent [@lew2010real]. Other morphosyntactic elements such as number are not as reliable as cues [@marull2017second], and for others, like case, the cue-outcome connection is directly not created [@hopp2015semantics; @mitsugi2016use].     
In morphosyntactic anticipation other factors might be at play, like L1 transfer and L2 proficiency. The workings of these factors start at lower levels of proficiency. @dussias2013gender examined L1 Italian and L1 English learners of L2 Spanish’s ability to use gender agreement as an anticipation cue. L1 Italian speakers could make use of the gender cue partially to make agreement anticipations at lower levels of proficiency, whereas L1 English speakers could not. L1 English speakers can only start to use gender as a cue at high-intermediate levels, under frequency or novelty conditions of the noun [@gruter2013l2; @lew2010real, respectively]. Italian speakers were presumably transferring their gender agreement knowledge from the L1 to the L2 in order to make the correct predictions. English speakers, however, lack this knowledge in their L1 so no extrapolation was possible. Additionally, it has been argued that lacking the representation of gender marking in the L1 might not only prevent prediction in an L2 based on that cue, but also hinder it [@hopp2016learning]. This proposal fits with @lew2010real findings that L1 English speakers at an intermediate level of proficiency cannot use gender marking to anticipate oncoming nouns in L2 Spanish, but they can use definiteness in articles to anticipate known nouns.     
While gender has been the cue more widely researched, but shared forms, number and case have also been included in past investigations. Having a similar system in terms of form can be helpful. @liburd2014investigating examined the abilities of beginning learners of L2 Dutch with English as L1 to use determiners with similar, different, and unique forms in English and Dutch to anticipate nouns. The eye-tracking data collected suggest that the English speakers were faster and more accurate in generating their predictions when the form was similar in their L1 and L2. Apart from gender, the influence of L2 proficiency is also visible in other cues such as number. Intermediate English speakers of L2 Spanish cannot use number to anticipate numbers suffix in a noun but advanced speakers can [@marull2017second]. In the case of case, the difficulty is never overcome, regardless of the L2 speaker's proficiency [@hopp2015semantics]. In Hopp's study, the L1 of the speakers was English. Like with gender, lacking a case system representation in the L1 might be preventing, and even hindering, the creation of case cue-suffix outcome connection.      
The conclusion that lack of L1 representation prevents L2 anticipation might only be applicable to morphosyntactic cases, or cases of lower proficiency, as it does not account for anticipatory behavior regarding phonology. @rehrig2017acoustic investigated whether L2 speakers could use suprasegmental information in a word stem like vowel lengthening to predict its suffix when their L1 lacked representation for those phenomena. They found that proficient L2 speakers did use the suprasegmental information, but learners at lower proficiencies did not. Similar studies focusing on tone have yielded similar results [e.g. @schremm2016implicit; @berthelsen2018neural].       
When the acoustic correlate does have a representation in the L1 but it differs from the one typical in the L2, results become messier. On the one hand, @sagarra2018suprasegmental found that L1 English speakers with advanced L2 Spanish could use lexical stress as a cue to anticipate verbal endings when the stress appeared in CVC syllables but not in CV syllables, and L2 Spanish beginners could not use stress to anticipate verb suffixes in either case. On the other hand, @dupoux2008persistent found that speakers of L1 French do not improve overtime in their discrimination of lexical stress in L2 Spanish, and hence cannot use it as an anticipatory cue. In contrast, Cantonese and Mandarin L1 speakers can learn to discriminate lexical stress in an L2 [@chen2013chinese; @li2017effects] and Korean L1 speakers do so to a certain extent [@hualde2015acquisition; @lee2019perception] even though neither of those languages require lexical stress encoding. Whether they can use lexical stress to anticipate other linguistic information has not been researched, although other suprasegmental cues such as tones [@hed2019neural; @schremm2016implicit] suggest that L2 speakers can learn to use suprasegmental cues in an L2 to anticipate linguistic morphosyntactic information, even if this learned ability does not reach L1-like performance [@perdomo2019prosodic]. Three models have been proposed as frameworks for phonological knowledge transfer.      
The models that have been proposed to account for cross-linguistic phonological effects were designed particularly with phoneme production in mind. However, it is possible to extend some of their tenets to provide a framework with which to study transfer and comprehension of suprasegmental information. Here I am going to describe those tenets relevant for this investigation. The three models are the Speech Learning Model [SLM, @flege1995second], the Perceptual Assimilation Model for L2 [PAM-L2, @best1995learning; @best2007nonnative], and the Second Language Linguistic Perception [L2LP, @escudero2005linguistic; @escudero2009linguistic; @leussen2015learning]. SLM focuses on ultimate attainment of L2 phonology and how age constraints target-like phonological succes in an L2. In this model, a speaker can exploit the same mechanisms and processes they used in the acquisition of their L1 phonological system to acquire the L2 sound system. The language-specific characteristics of sounds are encoded in the long-term memory, and the encoding and different speech categories in the long-term memory evolve with L1 and L2 knowledge to reflect the realization of sounds on both languages. Lastly, bilinguals differentiate the co-existing L1 and L2 phonetic categories. From these postulates, Flege elaborated on seven hypotheses regarding phoneme categorization and production in the L2. Two of these hypotheses are relevant for this project. First, a L1 phonetic category can block the creation of a similar category in the L2 due to equivalent classification. Therefore, the L1 phonetic category will be used to perceive sounds in both the L1 and the L2. Second, the L2 phonetic categories may present different characteristics from what L1 phonetic categories of than language present for two reasons, either the speaker is trying to keep the L2 category "deflected" from a category in their L1 in the common phonological space, or the L2 category in a bilingual speaker is characterized with different features or weights than the L1 category in a monolingual speaker.     
PAM-L2 was originally proposed to account for L1 phonological acquisition, but it later morphosed to account for L2 phonological development. Like in SLM, age of the speaker determines L2 perception learning, as well as other social factors, such as length of residence in an L2 environment. Importantly for this project, L2 perception in this model is going to be constrained by nonnative speech perception principles, that is, how similar or different the new sounds are compared to an already established sound system. L2 sounds can thus be assimilated to a L1 category, assimilated as uncategorizable to an existing sound, or discarded for being considered non-speech sounds. The last model is L2LP. This is the only one among the three models that was thought out to explain and predict L2 sound perception phenomena. In L2LP, L2 sounds are perceived through the L1 filter, that is, how they would be perceived if they were pronounced in the speaker's L1. Therefore, the acoustic similarities and discrepancies between the two phonological systems will shape the development of the encoding in the bilingual's mind. In L2 sound acquisition, a copy of the L1 system is created during the initial stages, this copy starts to adjust with exposure via the Gradual Learning Algorithm, a comparison of the L1 system and the perceived L2 sounds. The algorithm offers three possibilities: a new sound is assimilated to multiple L1 sounds, a new sound is perceived as similar to another one in the L1 system, or a new sound does not equal any category in the L1 so it requires a new category for itself. Recent revisions to the model have further proposed that perceiving is not the same thing as recognizing, but there is still not enough research in this direction to formulate any hypothesis.     
As a recap, the literature in L2 anticipation show that speakers can achieve some success in L2 anticipation depending on the cues and the context at more advanced levels of proficiency, but their performance will not be native-like [@sagarra2018suprasegmental; @gruter2016l2]. A possible explanation for the varied results on L2 perception and anticipation might be found in what speakers are transferring or extrapolating from their L1 that interacts with L2 new structures, such as the use of lexical stress. Whereas L2 speakers’ anticipation performance might depend on their ability to perceive the cues and what needs to be anticipated, asymmetries amongst studies and the lack of cognitive measures also difficult comparison of results.      
The lack of a common theoretical framework, the use of non-standardized measures to assess proficiency [self-assessment, @lew2010real], the variety of tasks [e.g., eye-tracking, @sagarra2018suprasegmental; vs. offline, @dupoux2008persistent], a variety of L1s [@hed2019neural], and the unclear distinction of variables [@schremm2016implicit] call for further research where the possible factors accounting for L2 anticipation patterns are better distinguished. To address the limitation of confounding the L1 transfer and L2 proficiency variables, @sagarra2018suprasegmental carried out a study where the possible transfer would be the same for all participants if there was any, and found that advanced learners of English could learn to use lexical stress as anticipatory cue in L2 Spanish. In that same study, working memory did not generally explain possible individual differences. The studies reviewed above point towards the impact that cross-linguistic associations may have on the ability to process and anticipate language in an L2. However, it is yet to be found out whether the ability to process in an L2 an unencoded or differently encoded prosodic element in the L1 also enables its use for linguistic anticipation. Additionally, it is yet to be found out whether speakers associate function knowledge or acoustic knowledge from the L1 encoded representations, and how this association interacts with models of phonetic transfer.     

2.2. Working memory
Working memory (WM) is the cognitive skill to store temporarily and process incoming information so complex cognitive actions are executed; WM is limited and its capacity varies from person to person [@baddeley2007working]. WM is deployed in all interactions with the world. It affects both linguistic and non-linguistic processing, and the interaction between different types of processing. As of today, many models have been proposed explaining how WM works.     
Three models stand out: @just1992capacity’s domain-specific single-resource model, @baddeley2007working’s domain-specific multiple-resource model, and domain-free connectionist models. In @just1992capacity’s model, language learning and processing is constrained by the WM, and there is a trade-off between the ability to process and to store linguistic information. This trade-off happens as a result of a competition for a shared pool of cognitive resources that need to be divided amongst the different actions required. Therefore, when a task depletes or overtax the WM of a person, their storage capacity may diminish and their processing slows down. In @baddeley2007working’s model linguistic performance is also constrained by WM, but the cognitive resources are organized differently. There is in this model a central executive that controls three subsystems: short-term storage phonological loop, short-term storage visuospatial sketchpad, and an episodic buffer. These three slave systems have independent limited capacities and are managed by the central executive. The two short-term memory systems focus on content, while the episodic buffer connects its sister systems with the long-term memory. The central executive coordinates the activity of the whole entity by filtering the information received, assembling information from different sources into meaningful episodes, regulating the flow of information among the subsystems, shifting tasks and shifting retrieval strategies. Lastly, connectionist models are domain-free as language capacities are determined by domain-general cognitive abilities. Specifically, by the ability to select which information to pay attention to and which information to inhibit, regardless of the nature of that information. In connectionist models there is no difference between processing and storage. Instead, WM is the activation of part of the long-term memory according to short-term patterns of activations related to domain-specific stores [e.g., @cowan2016working].        
WM gets deployed in a variety contexts, be them visual or auditory. In the visuospatial domain, WM is used for object searches. A successful and efficient search relies on spatial memory accuracy, such that when the spatial memory is busy with something else, search efficiency diminishes [@woodman2004visual]. When WM is totally invested in the search, a search for an object will be efficient even when the individual needs to remember a concurrent object at the same time [@woodman2001visual]. In the case of several stimuli in the visual field, processing capacity is biased towards those stimuli matching current objects already in the WM [@desimone1995neural], even when the cue in memory is held unconsciously [@pan2014working].     
WM also gets deployed in auditory or verbal modalities other than language. Previous research using a model like @baddeley2007working's suggests that children attending music classes score higher in phonological loop components tasks and central executive WM tasks than children who do not receive music training [@roden2014does]. In rhythmic synchronization, WM has been shown to be a reliable predictor of beat timing calculation in non-musicians [@colley2018working]. More precisely, @miyake2004two defend that anticipatory synchronization may be influenced by WM in the interstimulus-onset interval of 1800-3600 ms [@miyake2004two].      
Cross-domain research indicates that processing abilities in different areas are associated, and that better WM in one domain can produce effects in another. These effects would be a consequence of the distinct domains sharing span capabilities, while short-term capabilities are more unique to each domain [@kane2004generality]. In contrast, span capabilities are more structurally diverse than short-term capabilities are. In music, for instance, visual signals such as gestures promote better temporal anticipation and synchronizaiton of both musicians and non-musicians [@colley2018influence]. Children undergoing music training improve their simple spatial spans, forward and backward digit spans, non-word spans, and operation spans; and adults improve their digit and non-word span skills [@lee2007effects]. Furthermore, the improvement in both visual and auditory WM in children and teenagers is not a matter of experiment training, but it is proportional to the time spent on music practice per week in real music education over long periods of time [@bergman2014music]. The positive effect of music training on both the visual and the auditory domain also reflects on faster updating of WM and the re-allocation of neural resources to auditor stimuli [@george2011music].      
Empirical evidence of an association between different auditory and visual domains with language is provided by research on bilinguals. Bilinguals of two languages with an alphabetic system like English and a logosyllabic system like Chinese have enhanced visuospatial WM than bilinguals of two alphabetic systems, like English and Spanish [@ma2016working]. Observing bilinguals in general, both visual and auditory memory maintenance and manipulation capcities, and bilingual experience interact especially at an L2 intermediate level, while at advanced levels the interaction goes back to a more monolingual-like state [@yang2017bilinguals].     
The changes in interaction might be due to the progress of the L2 acquisition. At intermediate L2 levels, L2 acquisition is in full progress. The L2 has been activated enough to recruit WM to compensate for the lack of linguistic experience, and thus WM is enhanced. At lower levels of proficiency, the brain has not had time yet to create a response to L2 input; and at higher levels, linguistic experience is enough to process, understand and produce the L2, so the supplementation of WM is no longer needed.      
In language alone, WM is one of the main factors that may be determining individual differences in linguistic performance both in the L1 [e.g. @gathercole1990phonological] and the L2 [@huettig2016prediction found an effect; @otten2009does did not], but how profound its impact is depends on whether the language is being acquired, processed, or anticipated.      
Prior research on WM suggests that linguistic abilities in both the L1 and the L2 are importantly related to other non-linguistic abilities in language learning. For instance, @gathercole1992phonological demonstrated how important non-word repetition abilities were in L1 learning in children; and @gathercole1990phonological found that children with language disorders had lower WM that resulted in poorer performance in non-word repetition tasks in their L1. More generally, WM is essential in L1 comprehension regardless of the type of population to which the speaker belongs [@engle2002working]. These findings may as well be applied to L2 abilities: non-word repetition performance can predict L2 vocabulary and syntax acquisition success two years later in children [e.g. @cheung1996nonword; @gathercole1997phonological; @service1992phonology]. Some differences arise, though, when WM is too taxed or impaired and show symptoms only in learning an L2. For example, a person might have a phonological WM deficit and underperform only when learning words in a foreign language [@baddeley1988frontal], or when the cognitive load is too taxing, learning new words might be impeded only in foreign languages [@papagno1991phonological].      
WM may affect morphosyntactic L1 and L2 processing and anticipation as well as language acquisition. With regard to WM and L1 and L2 morphological and morphosyntactic processing, findings point towards different directions. In L1 morphological and morphosyntactic processing there is not much room for WM effects. @tanner2014erps found no association between WM and a dominance of N400 or P600 in morphosyntactic violations in limited semantic context. Likewise, @ye2008involvement found by means of EEG no effect of WM on reanalysis of voice violations either when there was more linguistic context. These authors did find, however, that participants with lower control abilities, implausible sentences produced positivity between 350 and 750 ms while plausible sentences provoked nothing regardless of the syntactic complexity. Individuals with higher control abilities, on the contrary, showed different activity according to syntactic complexity: anterior negativity between 350-600 ms for active implausible sentences, and positivity between 350-850 ms for passive implausible sentences. In a similar vein, @pakulak2010proficiency found that WM had no effect on the processing of  morphosyntactic violations in monolingual speakers of English with different socioeconomic status and level of education. Such findings suggest that individual differences in L1 processing are due to the contribution of other factors. WM has been indeed associated with L1 processing of semantic and syntactic information [@kim2018individual; @nakano2010speech], but any individual differences in morphological and morphosyntactic processing are conditioned by something else.     
In L2 morphosyntactic processing, results are not as clear-cut. Some studies have found facilitating effects of WM on morphosyntactic processing [e.g., @sagarra2016eye; @sanz2016one], while others have found no effect [e.g., @foote2011integrated; @grey2015role]. These differences might be explained by variability in L2 proficiency and the cognitive load imposed by the task and the WM test [@sagarra2017longitudinal]. For instance, speakers at a lower proficiency might be more susceptible to WM overload [@serafini2016evidence]. Therefore, WM differences might play a larger role in L2 morphological and morphosyntactic processing than they do in the L1. This role is likely to be also regulated by other factors, such as L1 transfer and L2 proficiency.     
Moving on to anticipation, findings on L1 anticipation show that typical L1 speakers tend to perform predictive language processing. How individual variability in cognitive capacities, i.e. WM, can affect the efficacy of the predictions is still unclear. @huettig2016prediction found through eye-tracking that the variance between participants in fixating on the correct target noun based on gendered article in Dutch could be accounted for through working memory. However, @sagarra2018suprasegmental found also by means of eye-tracking that working memory played no role, or a marginal one, in L1 Spanish speakers’ capacity to anticipate verbal tense based on lexical stress. Similarly, @otten2009does found no effect of working memory on L1 anticipation in an ERP study. These three studies researched the interaction of working memory and morphosyntax in different ways. In @huettig2016prediction's study morphosyntax was the cue to morphosyntax of a different word, and in @otten2009does's study morphosyntax was the cue too but the outcome was a whole noun coming after some adjectives in between. In return, morphosyntax was the outcome in @sagarra2018suprasegmental's study, but the cue there was phonological, and both were part of the same word. It is possible that the distance between the cue and the outcome, or the combination of morphology with other linguistic domains interacts with working memory.      
Regarding L2 anticipation, only @sagarra2018suprasegmental has looked into the influence of verbal WM on prediction in an L2. @sagarra2018suprasegmental studied anticipation of verbal tense morphosyntax when cued by lexical stress in L1 English learners of L2 Spanish, and found no effects, either in beginner or advanced learners. It is possible that WM differences, if they have any impact, only manifest in reduced contexts, such as morphology predicting morphology, or when WM interacts with other factors, such as L1 transfer or lack of a representation in the L1.     
To summarize, WM plays a role in language learning, processing, and anticipation, but the particular cases in which it exerts the influence, or the conditions that need to exist in order for the effects to be visible are unclear. While WM helps in predictive processing of morphology in the L1 [@huettig2016prediction], when larger [@otten2009does] or smaller elements [@sagarra2018suprasegmental] are involved, the influence is not so obvious. In the L2, WM effects fluctuate according to other factors like L2 proficiency [@serafini2016evidence], L1 transfer, or even anticipation abilities transferred from other related non-linguistic domains, such as rhythm, to account for predictive processing performance regarding morphology. However, this interaction has not been tested, so it is not possible to draw conclusions on why some studies report WM effects and not others.     

## 2.3. Linguistic phenomena
### 2.3.1. Lexical stress
Stress is the prominence of a syllable that speakers hear relative to the other syllables in the prosodic word [@hualde2005sounds]. The particular characteristics that define lexical stress, such as acoustic correlates or position within words, change drastically across languages. In English and Spanish, for example, lexical stress has no fixed position. Thus lexical stress is phonologically contrastive at the lexical level in both languages although to different degrees. That is, lexical stress can be used to distinguish between words, but the contrastive use is nevertheless much more typical in Spanish than in English. In English it is used predominantly to distinguish heteronyms or pairs of verbs-nouns that have no segmental differences. For instance, to “proDUCE,” verb vs. “PROduce,” noun. In Spanish, lexical stress differentiates all kinds of words and information, such as verbal tense and person ( _CANto_ 'I sing' vs. _canTÓ_ 's/he sang'), or nouns ( _PApa_ 'potato' vs. _paPÁ_ 'dad'), or nouns from verbs ( _TÉRmino_ 'term' vs. _terMIno_ 'I finish' vs. _termiNÓ_ 's/he finished').     
The acoustic realization of lexical stress is caused by different acoustic correlates depending on the language. Stress is the combined result of many parameters in action, among which we can find F0 variations, duration, overall intensity, and vowel formant frequencies [@gordon2017acoustic], and the different importances or weights assigned to each of these correlates cause the nature of stressed syllables in each language to vary. In Spanish, the most reliable cues to stress are pitch (F0), duration and intensity [@hualde2005sounds; @ortega2006phonetic; @ortega2007disentangling; @ortega2009perception]. Pitch is higher for stressed syllables and lower for unstressed syllables; regarding intensity, stressed syllables are louder; and lastly, stressed syllables are usually slightly longer. In contrast, the main cues in English are vowel duration and quality [@cooper2002constraints; @cutler1986forbear], although other cues such as intensity and pitch (F0) [@beckman1986stress; @fry1955duration; @fry1958experiments; @fry1965dependence; @sluijter1996spectral; @sluijter1997spectral] play minor roles. Thus, unstressed vowels are reduced to [ə].     
The different weight assigned to each cue in these languages may explain why L1 English speakers encounter difficulties in Spanish lexical stress perception [@face2006cognitive; @ortega2013english] and production [@lord2007role]. The different cue weights are also related to the prosodic structure of the language. Vowel reduction is linked to the rhythmic pattern of stressed-timed languages, like English, where the intervals between stressed syllables have similar durations. Spanish, on the contrary, is defined sometimes a syllable-timed language, as syllable duration is quite stable, and thus vowel trajectory length is approximately the same for all syllables regardless of their tonicity [@colantoni2015second; @hualde2005sounds]. In English, the stressed syllable signals a rhythmic unit that can be composed of multiple sub-units until the next stressed syllable and thus rhythmic unit arrives; while in Spanish, stress is simply part of a syllable, and each syllable is a new rhythmic unit.    
The different weights assigned to each cue in different languages interact with lexical stress processing particular to each language. As mentioned above, lexical stress helps activation of lexical entries in L1 Spanish [@soto2001segmental], such that a prosodically matching cue to the target ( _prinCI_ > _prinCIpio_ 'start') results in shorter and more accurate decision making times, when compared to mismatching cues ( _PRINci_ > _prinCIpio_ 'start'). These results are taken to suggest that participants in @soto2001segmental study were anticipating the lexical element based on suprasegmental cues such as lexical stress. Contrarily, it is unclear whether L1 English speakers are able to use stress alone as a cue to anticipate and facilitate lexical activation. On the one hand, @cooper2002constraints tested L1 English speakers in a similar study to that of @soto2001segmental and found that the English natives were only able to use the suprasegmental cues when more than one syllable of the word was present. On the other hand, @perdomo2019prosodic did find in an eye-tracking study that the presence of lexical stress elicited fixations on the oncoming target noun. It is possible that the L1 English speakers were using the relative low emphasis in previous syllables to activate the cue role for lexical stress in the syllables that were perceptually more prominent; similarly to what the L1 Spanish might have done, as the target words came at the end of a context sentence. This difference in performance amongst studies is probably due to what cues speakers use to discriminate lexical stress, and how these cues are instantiated in the language. That is, English L1 speakers may be placing a larger reliance on duration to process Spanish lexical stress. This reliance would be transferred from L1 English processing. Spanish L1 speakers may be relying more on other cues, such as pitch and intensity, that are discarded by the English speakers because they are not used to resorting to them, and that are more prominent in the language as compared to the one English L1 speakers are using [@ortega2013english].    
The differences and similarities in cue weighting can no doubt influence lexical stress perception in an L2. For example, @cooper2002constraints found that the similar distribution of stress in Dutch and English helped L1 Dutch learners of English transfer their knowledge of lexical stress to process it properly in L2 English. But, in contrast, German speakers have more difficulty perceiving stress in another free lexically stressed language such as Spanish than L1 speakers [@schwab2016use]; again, maybe due to the acoustic correlates they use to discriminate the suprasegmental. Following this line, @vickie2010cross suggested that language background in the L1 can affect how lexical stress is perceived in an L2 and what correlates are used to discriminate the L2 stress.    
The studies above suggest that the acoustic properties of prosody are essential in processing and activating language. The importance speakers assign to each cue might extend beyond perception and affects anticipation as well. Studies like the one by @vickie2010cross further suggest that speakers can resort to their prosodic abilities in the L1 in order to process other prosodic structures in their L2 absent in their L1, such that Chinese speakers can use pitch knowledge to discriminate lexical stress in Spanish. Extending this hypothesis, L2 speakers might be able to extrapolate acoustic knowledge and reassemble it into new prosodic structures that are encoded in the L2 lexicon and use it as cues for linguistic anticipation. Specifically, tonal language speakers might be able to transfer pitch knowledge to an L2 to encode lexical stress based primarily on pitch along with the word to which it belongs. After lexical stress has been encoded in the L2 lexicon thanks to L1 pitch knowledge, L2 speakers might learn to use this new prosodic structure as the basis for L2 prediction so as to reduce the processing cognitive load.

### 2.3.2. Tone
Tones are the pitch contour patterns of the voiced part in syllables [@chao1968grammar]. Many languages use tones, or changes in pitch-contour, at a phrasal level for pragmatic purposes. However, only a few use tones contrastively at a lexical level. The acoustic correlates for tones vary across languages: some use only pitch (e.g., Mandarin Chinese), whereas others also use length and/or register (e.g., Cantonese Chinese). Relevant to my dissertation with Mandarin Chinese speakers, in most Mandarin Chinese dialects (e.g., from Beijing and Tianjin), the main acoustic correlate for tones is changes in pitch (F0) contour or changes in pitch height within a syllable [@gandour1978perception; @zhu2015tone]. Importantly, tones in Mandarin Chinese do not cause shorter and longer syllables. Therefore, Mandarin is described as a syllable-timed language in terms of rhythm [@grabe2002durational; @lin2007mandarin; @mok2009syllable], just like Spanish.     
In Mandarin, tones facilitate word recognition [@malins2010roles]. They are nevertheless not the most important factor in that process, as vowel especially but also consonant identity comes first [@hu2012dissociation]. In other words, while tones confirm that the correct word is activated, the main vehicle to access a lexicon entry in Mandarin are other cues, mainly segmental cues. @wiener2016constraints investigated the degree to which segmental (vocalic and consonantal phonemes) and suprasegmental cues (tones) constrained lexical access in Mandarin Chinese. L1 Mandarin speakers were presented different types of stimuli containing different types of violations (tonotactic, phonactic) and had to decide as fast as possible if what they were hearing was a real word or not. Words with tonotactic violations were more often endorsed as real words than other types of violations, such as vowel or consonant change. These results led the authors to conclude that tone information is not as important as consonant and especially vowel information in lexical access. @hu2012dissociation reached a similar conclusion in a ERP study. @hu2012dissociation studied the relevance of tone and vowel information at different stages of lexical access, for which they selected fixed Chinese idioms and isolated words as context. Vowel mismatches evoked earlier (N1 effect) than tone mismatches (N400). The N1 effect was taken to suggest that vowel was playing a role on word selection, while the N400 effect signals a failure of the integration of the word in the sentential context. @sereno2015contribution obtained similar results in two priming experiments in Mandarin. Participants were slower in discriminating words where the only difference was tonal. Primings where both vowel and tone matched where the fastest one, followed by primings where only vowel matched. The three studies led to the conclusion that the functions of vowels and tones in Mandarin are distinct. Namely, that vowel plays a major role in activation, while the role of tone is integration in the higher context.       
The knowledge of the nature and function of tones in the L1 can affect L2 tone learning positively by providing a background knowledge to which learners can resort to acquire the L2 tones. However, L1 tone knowledge can also affect L2 tone learning negatively when the association with the L1 tone knowledge interferes with the nature of the L2 tones. @li2017effects examined the influence of the L1 tonal knowledge in the acquisition of L2 tones in children. These children were L1 Cantonese speakers learning L2 Mandarin, and they had issues in categorizing Mandarin tones 1 and 4, as these tones would be assimilated to the same tone 1 category in Cantonese. In the case of these children, being a native speaker to a tonal language helped them in the perception of Mandarin tones 2 and 3, but it hindered perception of other L2 tones because the knowledge association with L1 tones disagreed from the L2 tone structure and interfered with it.     
Although it seems the role of tones is not as pre-eminent of lexical stress in Spanish, it still helps in word activation and must be encoded in the lexicon. Mandarin speakers need to pay attention to the pitch variations in order to assign the correct tone to the word they are hearing. Since pitch variations are the basis for lexical stress in Spanish, Mandarin speakers might be able to extrapolate their sensitivity to pitch changes to process and use pitch to anticipate linguistic information more easily than English speakers. In English lexical stress, pitch variation is not as important as an acoustic correlate so the information it contains in an L2 is mostly overlooked by L1 English speakers. For L1 English speakers, learning to distinguish the pitch variations may be more difficult than simply extrapolating the sensitivity, and thus Mandarin speakers may outperform English speakers in using lexical stress to anticipate verbal tense in L2 Spanish.     
Lexical stress and tones are different prosodic structures with functions that may differ or not across languages. Lexical stress can be used contrastively at the lexical level, like in English, or it might not, like in French. Likewise, tones can also be contrastive, like in Mandarin. They are different prosodic structures, and lacking a representation in the L1 can hamper their acquisition in an L2. They can share, however, their acoustic correlates to some extent. Both structures make use of correlates like pitch or vowel quality to perform their function. Extrapolating this knowledge can be helpful in processing sounds in a different language. But how effective and helpful this knowledge extrapolation is might depend on the interplay of other factors, such as working memory, L2 proficiency, or other auditory abilities. 